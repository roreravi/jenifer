{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7e65e22",
   "metadata": {},
   "source": [
    "Clase 2: Carga, Lectura y Exploración Detallada de DataFrames \n",
    "\n",
    "Objetivo de la Clase: Que la alumna domine las opciones de carga de archivos CSV, entienda la importancia de los tipos de datos en Spark, y sea capaz de realizar una exploración profunda de los DataFrames utilizando métodos clave para comprender la estructura y las características de los datos bancarios.\n",
    "\n",
    "Contenido Detallado:\n",
    "\n",
    "Repaso Rápido de SparkSession y Carga Básica\n",
    "\n",
    "Recordatorio de cómo inicializar la SparkSession.\n",
    "Repaso de la carga simple: spark.read.csv(\"ruta\", header=True, inferSchema=True).\n",
    "\n",
    "Opciones Avanzadas al Leer CSV\n",
    "\n",
    "header=True/False: Ya visto, pero reconfirmar su importancia para los nombres de columnas.\n",
    "inferSchema=True/False:\n",
    "Ventajas de True: Conveniente para explorar datos rápidamente.\n",
    "Desventajas de True: Puede ser lento para archivos muy grandes (Spark necesita una pasada completa para inferir), y a veces infiere tipos incorrectos (ej. números como strings).\n",
    "Cuando usar False: En producción o con archivos gigantes, es mejor definir el esquema manualmente.\n",
    "sep (separador): Especifica el delimitador de las columnas (ej. sep=\";\" para archivos punto y coma).\n",
    "nullValue: Define qué cadena de texto debe interpretarse como null (ej. nullValue=\"N/A\").\n",
    "dateFormat y timestampFormat: Crucial para datos bancarios. Cómo especificar el formato de fecha para que Spark lo parsee correctamente a tipo Date o Timestamp.\n",
    "Ej. dateFormat=\"yyyy-MM-dd\", timestampFormat=\"yyyy-MM-dd HH:mm:ss\".\n",
    "Esquema Definido Manualmente (Programmatic Schema):\n",
    "La mejor práctica para robustez y rendimiento.\n",
    "Se crea un objeto StructType con StructField para cada columna.\n",
    "Define el nombre de la columna, el tipo de dato (StringType, IntegerType, DoubleType, DateType, TimestampType de pyspark.sql.types), y si es nullable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "018accac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession creada con éxito.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Crear una SparkSession\n",
    "# .builder: Inicia el constructor de la sesión.\n",
    "# .appName(\"Nombre de la Aplicacion\"): Asigna un nombre a tu aplicación Spark.\n",
    "# .config(\"spark.executor.memory\", \"4g\"): Configura propiedades de Spark (ej. memoria de los ejecutores).\n",
    "# .getOrCreate(): Crea una nueva SparkSession si no existe, o devuelve la existente.\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CursoPySparkBancario_Clase2\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"SparkSession creada con éxito.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ff7b9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType\n",
    "\n",
    "# Ejemplo de esquema para bank_data.csv\n",
    "bank_schema = StructType([\n",
    "    StructField(\"Branch_ID\", IntegerType(), True),\n",
    "    StructField(\"City\", StringType(), True),\n",
    "    StructField(\"Region\", StringType(), True),\n",
    "    StructField(\"Firm_Revenue\", DoubleType(), True),\n",
    "    StructField(\"Expenses\", DoubleType(), True), # Cambiado a Double por consistencia con Profit_Margin\n",
    "    StructField(\"Profit_Margin\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Cargar usando el esquema definido\n",
    "df_bank_manual = spark.read.csv(\"/home/ravi/Documents/jenifer/data/bank_data.csv\", header=True, schema=bank_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bd4df6",
   "metadata": {},
   "source": [
    "Discusión: Ventajas de usar esquema manual para control de calidad y rendimiento, especialmente en un banco donde la consistencia de datos es vital.\n",
    "\n",
    "Tipos de Datos en Spark y su Importancia \n",
    "\n",
    "Repaso de tipos comunes: StringType, IntegerType, DoubleType, BooleanType, DateType, TimestampType.\n",
    "¿Por qué son importantes?\n",
    "Exactitud de cálculos: Operaciones aritméticas solo funcionan con tipos numéricos.\n",
    "Uso de memoria: Tipos correctos optimizan el almacenamiento.\n",
    "Funciones específicas: Las funciones de fecha (year, month, dayofmonth) solo funcionan con tipos DateType o TimestampType.\n",
    "Calidad de datos: Un tipo de dato incorrecto puede indicar un problema subyacente en los datos fuente.\n",
    "Casting (Cambio de Tipo): Cómo convertir una columna de un tipo a otro (.cast())."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "657f2ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df_bank_manual = df_bank_manual.withColumn(\"Branch_ID_str\", col(\"Branch_ID\").cast(StringType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bb7e2a",
   "metadata": {},
   "source": [
    "Métodos Esenciales de Exploración Detallada\n",
    "\n",
    "printSchema(): (Repaso) Verificación rápida de la estructura.\n",
    "show(): (Repaso) Inspección visual de filas.\n",
    "describe(): Obtiene estadísticas descriptivas (count, mean, stddev, min, max) para columnas numéricas y strings. Muy útil para un primer vistazo a la distribución de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91a4c59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/09 12:59:49 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+-------------+--------+------+---------+-----------------+\n",
      "|summary|       Customer_ID|               Age|Customer_Type|    City|Region|Bank_Name|        Branch_ID|\n",
      "+-------+------------------+------------------+-------------+--------+------+---------+-----------------+\n",
      "|  count|             10000|              9500|         9500|    9500| 10000|    10000|            10000|\n",
      "|   mean|          204999.5| 48.75442105263158|         null|    null|  null|     null|        1497.8293|\n",
      "| stddev|2886.8956799071675|17.919178673847643|         null|    null|  null|     null|288.7582371006225|\n",
      "|    min|            200000|              18.0|     Business|Kolhapur|  East|HDFC Bank|             1000|\n",
      "|    max|            209999|              79.0|   Individual| Solapur|  West|HDFC Bank|             1999|\n",
      "+-------+------------------+------------------+-------------+--------+------+---------+-----------------+\n",
      "\n",
      "+-------+------------------+------------------+\n",
      "|summary|     Total_Balance|Transaction_Amount|\n",
      "+-------+------------------+------------------+\n",
      "|  count|             10000|             10000|\n",
      "|   mean|        50221.5058| 2542.708056138515|\n",
      "| stddev|28540.392010807747|1432.6774203740142|\n",
      "|    min|             10002|             100.0|\n",
      "|    max|             99993|             999.0|\n",
      "+-------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_customer = spark.read.csv(\"/home/ravi/Documents/jenifer/data/customer_data.csv\", header=True)\n",
    "df_customer.describe().show()\n",
    "df_transaction = spark.read.csv(\"/home/ravi/Documents/jenifer/data/transaction_data.csv\", header=True)\n",
    "df_transaction.select(\"Total_Balance\", \"Transaction_Amount\").describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4264d6c",
   "metadata": {},
   "source": [
    "summary(): Similar a describe(), pero ofrece más estadísticas como cuartiles, IQR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ce25c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------+------------------+------------------+------------------+-----------------+----------------+\n",
      "|summary|    Transaction_ID|       Customer_ID|Account_Type|     Total_Balance|Transaction_Amount| Investment_Amount|  Investment_Type|Transaction_Date|\n",
      "+-------+------------------+------------------+------------+------------------+------------------+------------------+-----------------+----------------+\n",
      "|  count|             10000|             10000|       10000|             10000|             10000|             10000|            10000|           10000|\n",
      "|   mean|          304999.5|       205028.1826|        null|        50221.5058| 2542.708056138515|        25550.2484|             null|            null|\n",
      "| stddev|2886.8956799071675|2909.0081392963934|        null|28540.392010807747|1432.6774203740142|14108.052077783103|             null|            null|\n",
      "|    min|            300000|            200000|    Business|             10002|             100.0|             10001|    Fixed Deposit|      2022-03-21|\n",
      "|    25%|          302498.0|          202492.0|        null|           25131.0|            1309.0|           13376.0|             null|            null|\n",
      "|    50%|          304998.0|          205059.0|        null|           50318.0|            2538.0|           25694.0|             null|            null|\n",
      "|    75%|          307498.0|          207558.0|        null|           74910.0|            3746.0|           37730.0|             null|            null|\n",
      "|    max|            309999|            209999|     Savings|             99993|             999.0|              9994|Recurring Deposit|      2025-03-20|\n",
      "+-------+------------------+------------------+------------+------------------+------------------+------------------+-----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transaction.summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92e7c9b",
   "metadata": {},
   "source": [
    "count(): (Repaso) Número total de filas.\n",
    "columns / dtypes: Lista de nombres de columnas / tuplas (nombre, tipo de dato).\n",
    "distinct() y countDistinct(): Para explorar valores únicos.\n",
    "df.select(\"Region\").distinct().show(): Muestra los valores únicos en una columna.\n",
    "df.select(countDistinct(\"Region\")).show(): Cuenta el número de valores únicos. (Requiere from pyspark.sql.functions import countDistinct).\n",
    "groupBy().count() y groupBy().agg(): Para ver la distribución de valores en columnas categóricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89947306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|Customer_Type|count|\n",
      "+-------------+-----+\n",
      "|         null|  500|\n",
      "|   Individual| 3132|\n",
      "|     Employee| 3182|\n",
      "|     Business| 3186|\n",
      "+-------------+-----+\n",
      "\n",
      "+------+-----+\n",
      "|Region|count|\n",
      "+------+-----+\n",
      "| South|  256|\n",
      "|  East|  265|\n",
      "|  West|  239|\n",
      "| North|  240|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_customer.groupBy(\"Customer_Type\").count().show()\n",
    "df_bank_manual.groupBy(\"Region\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c33f75",
   "metadata": {},
   "source": [
    "select(): (Repaso) Selección de columnas.\n",
    "filter() / where(): (Repaso) Filtrado de filas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9203b329",
   "metadata": {},
   "source": [
    "Ejemplos Prácticos en Clase (con el instructor):\n",
    "\n",
    "Cargar customer_data.csv y transaction_data.csv:\n",
    "Primero con inferSchema=True, mostrar printSchema().\n",
    "Identificar Customer_Type como string y Age como double.\n",
    "Identificar Transaction_Date como string en transaction_data.csv (¡problema común!).\n",
    "Definir Esquema Manual para transaction_data.csv:\n",
    "Construir el StructType para transaction_data.csv, asegurándose de que Transaction_Date sea DateType.\n",
    "Recargar el DataFrame con el esquema manual y mostrar printSchema() de nuevo para ver la corrección.\n",
    "Discutir por qué esto es más robusto para un banco.\n",
    "Uso de describe() y summary():\n",
    "Ejecutar df_customer.describe().show() y analizar las estadísticas para Age.\n",
    "Ejecutar df_transaction.select(\"Total_Balance\", \"Transaction_Amount\").describe().show().\n",
    "Comparar describe() y summary().\n",
    "Explorar valores únicos y distribuciones:\n",
    "Mostrar df_customer.select(\"Customer_Type\").distinct().show().\n",
    "Contar tipos de cliente: df_customer.groupBy(\"Customer_Type\").count().show().\n",
    "Contar bancos únicos: df_customer.groupBy(\"Bank_Name\").count().show().\n",
    "Casting de columna: Convertir Branch_ID en df_bank a StringType como ejemplo de cast()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3367dccd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80d70ade",
   "metadata": {},
   "source": [
    "10 Ejercicios para la Estudiante (Requerimientos Laborales Simulados):\n",
    "\n",
    "Contexto General: Eres un especialista en datos en el Departamento de Análisis de Clientes del Banco y el Departamento de Transacciones, encargado de asegurar la calidad y el entendimiento de nuestros datos.\n",
    "\n",
    "Archivos a Usar: customer_data.csv y transaction_data.csv.\n",
    "\n",
    "Instrucciones: Para cada ejercicio, crea una nueva SparkSession si no la tienes activa, o usa la existente. Asegúrate de importar las librerías necesarias.\n",
    "\n",
    "Inicialización Robusta de Datos de Clientes:\n",
    "Requerimiento: Nuestro equipo de desarrollo de ETL necesita una carga confiable de los datos de clientes. El tipo de cliente (Customer_Type) a veces viene con el valor \"N/A\" que debe tratarse como nulo, y la edad (Age) a veces no se registra.\n",
    "Ejercicio: Carga customer_data.csv en un DataFrame llamado df_clientes. Asegúrate de que N/A en cualquier columna se interprete como nulo y que Spark infiera los tipos de datos. Muestra el esquema.\n",
    "Preparación del Esquema de Transacciones:\n",
    "Requerimiento: Las fechas de transacción (Transaction_Date) son críticas para la auditoría y los reportes financieros. Necesitamos asegurarnos de que Spark las reconozca explícitamente como fechas (DateType) con el formato AAAA-MM-DD para evitar errores.\n",
    "Ejercicio: Define un StructType para transaction_data.csv donde Transaction_Date sea DateType con el formato yyyy-MM-dd. Carga transaction_data.csv en un DataFrame llamado df_transacciones usando este esquema. Muestra el esquema resultante.\n",
    "Validación de Carga de Transacciones:\n",
    "Requerimiento: El equipo de Riesgos necesita una confirmación rápida del volumen de transacciones que hemos procesado.\n",
    "Ejercicio: Muestra el número total de transacciones en df_transacciones.\n",
    "Análisis Descriptivo de Saldos:\n",
    "Requerimiento: El equipo de Gestión de Cartera quiere conocer las estadísticas básicas (mínimo, máximo, promedio, etc.) de los saldos totales (Total_Balance) y los montos de inversión (Investment_Amount) para entender la distribución de activos de nuestros clientes.\n",
    "Ejercicio: Realiza un describe() o summary() solo sobre las columnas Total_Balance y Investment_Amount de df_transacciones y muestra los resultados.\n",
    "Perfiles de Clientes por Edad:\n",
    "Requerimiento: El equipo de Marketing está interesado en la distribución de edades de nuestros clientes para segmentar campañas.\n",
    "Ejercicio: Utiliza describe() sobre la columna Age del df_clientes para obtener sus estadísticas descriptivas.\n",
    "Diversidad de Tipos de Cuenta:\n",
    "Requerimiento: El Departamento de Productos Bancarios quiere saber cuántos tipos de cuentas bancarias diferentes manejamos y cuáles son.\n",
    "Ejercicio: Muestra todos los valores únicos de la columna Account_Type en df_transacciones.\n",
    "Conteo de Clientes por Tipo:\n",
    "Requerimiento: Marketing también quiere saber cuántos clientes tenemos en cada Customer_Type (ej. Employee, Business, Individual).\n",
    "Ejercicio: Agrupa df_clientes por Customer_Type y cuenta el número de clientes en cada grupo.\n",
    "Verificación de Nombres de Bancos:\n",
    "Requerimiento: Necesitamos una lista de todos los nombres de bancos que aparecen en nuestros datos de clientes para una auditoría.\n",
    "Ejercicio: Muestra los nombres únicos de los bancos (Bank_Name) presentes en df_clientes.\n",
    "Identificación de Transacciones con Valores Extremos:\n",
    "Requerimiento: El equipo de Fraude está realizando una revisión y pide identificar cualquier transacción con un Transaction_Amount inusualmente alto (ej. mayor a 4000).\n",
    "Ejercicio: Filtra df_transacciones para mostrar solo las transacciones donde Transaction_Amount sea mayor que 4000. Muestra las primeras 5 de estas transacciones.\n",
    "Preparación para Próxima Clase (Recarga y Conversión):\n",
    "Requerimiento: Para futuras transformaciones, necesitamos asegurarnos de que Branch_ID en df_clientes sea del tipo IntegerType (si no lo es ya por el inferSchema).\n",
    "Ejercicio: Si Branch_ID en df_clientes no es IntegerType, conviértelo a IntegerType usando .cast() y .withColumn(). Muestra el printSchema() para confirmar el cambio. Detén tu SparkSession al finalizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c58af9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d541685e",
   "metadata": {},
   "source": [
    "Recursos Adicionales para la Alumna:\n",
    "\n",
    "Documentación de pyspark.sql.types: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.types.StructType.html\n",
    "Documentación de pyspark.sql.DataFrameReader (para opciones de lectura): https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.csv.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
