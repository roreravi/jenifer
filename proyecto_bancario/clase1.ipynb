{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c14a060",
   "metadata": {},
   "source": [
    "Contenido Detallado:\n",
    "\n",
    "¿Qué es Apache Spark y por qué es fundamental en Big Data?\n",
    "\n",
    "Contexto: En el sector bancario, se manejan volúmenes masivos de datos: transacciones, clientes, préstamos, inversiones, datos de mercado, etc. Las herramientas tradicionales de procesamiento de datos a menudo no son suficientes.\n",
    "Introducción a Spark: Un motor de procesamiento de datos distribuido y de propósito general, diseñado para análisis de Big Data.\n",
    "Características clave:\n",
    "Velocidad: Hasta 100x más rápido que Hadoop MapReduce en memoria y 10x en disco, ideal para análisis en tiempo casi real.\n",
    "Fácil de usar: APIs en Python (PySpark), Scala, Java, R y SQL.\n",
    "Generalidad: Soporta ETL, SQL, machine learning, procesamiento de gráficos y streaming.\n",
    "Flexibilidad: Puede ejecutarse en Hadoop YARN, Apache Mesos, Kubernetes, o de forma independiente.\n",
    "Importancia para el banco: Procesamiento rápido de millones de transacciones, detección de fraude, análisis de riesgo, personalización de ofertas a clientes, reportes regulatorios complejos.\n",
    "\n",
    "PySpark: La Interfaz de Python para Spark \n",
    "\n",
    "¿Por qué Python? Popularidad, facilidad de uso, gran ecosistema de librerías para ciencia de datos y machine learning.\n",
    "PySpark: Permite a los desarrolladores de Python interactuar con el motor Spark. Combina la potencia de Spark con la simplicidad de Python.\n",
    "Componentes principales en PySpark:\n",
    "SparkContext: El punto de entrada \"antiguo\" para funcionalidades de bajo nivel (RDDs).\n",
    "SparkSession: El punto de entrada \"moderno\" y unificado. Recomendado para trabajar con DataFrames y SQL.\n",
    "\n",
    "RDDs vs. DataFrames (Énfasis en DataFrames) \n",
    "\n",
    "RDDs (Resilient Distributed Datasets):\n",
    "Estructura de datos fundamental de Spark.\n",
    "Colección de objetos inmutables y distribuidos.\n",
    "Ofrecen control de bajo nivel.\n",
    "Menos optimizados para datos estructurados.\n",
    "DataFrames:\n",
    "Colección de datos distribuidos organizados en columnas con nombre (similar a tablas de bases de datos o data frames de Pandas/R).\n",
    "Ventajas clave:\n",
    "Optimización automática: Spark puede optimizar las operaciones de DataFrames usando el \"Catalyst Optimizer\" y \"Project Tungsten\". Esto significa que son mucho más eficientes que los RDDs para datos estructurados.\n",
    "Familiaridad: Fácil de usar para quienes conocen SQL o Pandas.\n",
    "Esquema: Tienen un esquema bien definido (nombres de columnas y tipos de datos), lo que permite a Spark aplicar optimizaciones.\n",
    "Conclusión: Para la ingeniería de datos en el banco, donde la mayoría de los datos son estructurados (transacciones, clientes, sucursales), los DataFrames son la elección ideal y serán el foco principal del curso.\n",
    "\n",
    "Configuración Básica de una Sesión de Spark (SparkSession)\n",
    "\n",
    "Concepto: SparkSession es el punto de entrada principal para programar Spark con la API de DataFrame y SQL.\n",
    "Importaciones esenciales: from pyspark.sql import SparkSession.\n",
    "Construyendo una SparkSession:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e900af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/09 12:13:53 WARN Utils: Your hostname, ravi-lap resolves to a loopback address: 127.0.1.1; using 192.168.1.108 instead (on interface wlp2s0)\n",
      "25/06/09 12:13:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/09 12:13:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession creada con éxito.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Crear una SparkSession\n",
    "# .builder: Inicia el constructor de la sesión.\n",
    "# .appName(\"Nombre de la Aplicacion\"): Asigna un nombre a tu aplicación Spark.\n",
    "# .config(\"spark.executor.memory\", \"4g\"): Configura propiedades de Spark (ej. memoria de los ejecutores).\n",
    "# .getOrCreate(): Crea una nueva SparkSession si no existe, o devuelve la existente.\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CursoPySparkBancario_Clase1\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"SparkSession creada con éxito.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12de65c1",
   "metadata": {},
   "source": [
    "Detener la SparkSession: spark.stop() (importante para liberar recursos al finalizar el trabajo).\n",
    "Entornos de trabajo:\n",
    "Local: Para aprendizaje y desarrollo en tu máquina.\n",
    "Google Colab con PySpark: Una excelente opción gratuita para practicar sin configuraciones complejas.\n",
    "Databricks Community Edition: Plataforma basada en la nube, ideal para escenarios más cercanos a la producción. (Mencionar, no profundizar en la configuración aquí).\n",
    "Jupyter Notebooks: Común para desarrollo local.\n",
    "\n",
    "Carga y Exploración Inicial de Datos en un DataFrame\n",
    "\n",
    "Preparación de los datos: Asegurarse de que bank_data.csv esté accesible (en el mismo directorio, o subirlo a Colab/Databricks).\n",
    "Cargar un archivo CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16a35d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suponiendo que 'bank_data.csv' está en la misma carpeta o ruta especificada\n",
    "df_bank = spark.read.csv(\"/home/ravi/Documents/jenifer/data/bank_data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# header=True: Indica que la primera fila es el encabezado.\n",
    "# inferSchema=True: Spark intentará adivinar los tipos de datos de las columnas.\n",
    "# (¡Precaución con inferSchema en producción por rendimiento!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097d4395",
   "metadata": {},
   "source": [
    "Exploración inicial:\n",
    "printSchema(): Muestra el esquema (nombres de columnas y tipos de datos inferidos). Esto es crucial para entender cómo Spark interpreta tus datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dc03a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Branch_ID: integer (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- Firm_Revenue: double (nullable = true)\n",
      " |-- Expenses: integer (nullable = true)\n",
      " |-- Profit_Margin: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_bank.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140f2c6c",
   "metadata": {},
   "source": [
    "show(): Muestra las primeras filas del DataFrame. Por defecto, muestra 20 filas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6c4ec19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------+------------+--------+-------------+\n",
      "|Branch_ID|    City|Region|Firm_Revenue|Expenses|Profit_Margin|\n",
      "+---------+--------+------+------------+--------+-------------+\n",
      "|     1000| Solapur|  East|    422443.0|  346471|        56.31|\n",
      "|     1001|  Mumbai| North|    211155.0|  216256|        79.53|\n",
      "|     1002|  Nashik|  West|    985006.0|   71777|        55.31|\n",
      "|     1003| Solapur| North|     55113.0|  253655|         2.57|\n",
      "|     1004|  Nagpur|  West|    282701.0|  148194|        -14.1|\n",
      "|     1005|  Nagpur|  East|    657164.0|   90729|        11.57|\n",
      "|     1006|Kolhapur| North|     96520.0|  273124|        19.48|\n",
      "|     1007|Kolhapur|  West|    427163.0|  111053|        -9.09|\n",
      "|     1008|Kolhapur| North|        null|   59920|        -5.94|\n",
      "|     1009|  Nagpur| South|    690932.0|   44992|        56.47|\n",
      "|     1010| Solapur| North|    936640.0|  226126|        -18.9|\n",
      "|     1011|  Nashik|  East|    257505.0|  483161|        65.12|\n",
      "|     1012|  Nagpur|  West|    969777.0|  347637|        -11.4|\n",
      "|     1013|    Pune|  West|    214437.0|   52323|        10.97|\n",
      "|     1014|  Nagpur| North|    370373.0|  194199|        -8.67|\n",
      "|     1015|  Nagpur|  East|    494847.0|  394802|        73.24|\n",
      "|     1016|  Nashik| North|    366811.0|  326818|         9.12|\n",
      "|     1017|  Nagpur|  East|    658552.0|  280743|        96.78|\n",
      "|     1018|  Nagpur|  East|    938738.0|  218097|        40.11|\n",
      "|     1019|  Nashik|  West|    112412.0|  219553|        -47.5|\n",
      "+---------+--------+------+------------+--------+-------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------+-------+------+------------+--------+-------------+\n",
      "|Branch_ID|   City|Region|Firm_Revenue|Expenses|Profit_Margin|\n",
      "+---------+-------+------+------------+--------+-------------+\n",
      "|     1000|Solapur|  East|    422443.0|  346471|        56.31|\n",
      "|     1001| Mumbai| North|    211155.0|  216256|        79.53|\n",
      "|     1002| Nashik|  West|    985006.0|   71777|        55.31|\n",
      "|     1003|Solapur| North|     55113.0|  253655|         2.57|\n",
      "|     1004| Nagpur|  West|    282701.0|  148194|        -14.1|\n",
      "+---------+-------+------+------------+--------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------+--------+------+------------+--------+-------------+\n",
      "|Branch_ID|City    |Region|Firm_Revenue|Expenses|Profit_Margin|\n",
      "+---------+--------+------+------------+--------+-------------+\n",
      "|1000     |Solapur |East  |422443.0    |346471  |56.31        |\n",
      "|1001     |Mumbai  |North |211155.0    |216256  |79.53        |\n",
      "|1002     |Nashik  |West  |985006.0    |71777   |55.31        |\n",
      "|1003     |Solapur |North |55113.0     |253655  |2.57         |\n",
      "|1004     |Nagpur  |West  |282701.0    |148194  |-14.1        |\n",
      "|1005     |Nagpur  |East  |657164.0    |90729   |11.57        |\n",
      "|1006     |Kolhapur|North |96520.0     |273124  |19.48        |\n",
      "|1007     |Kolhapur|West  |427163.0    |111053  |-9.09        |\n",
      "|1008     |Kolhapur|North |null        |59920   |-5.94        |\n",
      "|1009     |Nagpur  |South |690932.0    |44992   |56.47        |\n",
      "|1010     |Solapur |North |936640.0    |226126  |-18.9        |\n",
      "|1011     |Nashik  |East  |257505.0    |483161  |65.12        |\n",
      "|1012     |Nagpur  |West  |969777.0    |347637  |-11.4        |\n",
      "|1013     |Pune    |West  |214437.0    |52323   |10.97        |\n",
      "|1014     |Nagpur  |North |370373.0    |194199  |-8.67        |\n",
      "|1015     |Nagpur  |East  |494847.0    |394802  |73.24        |\n",
      "|1016     |Nashik  |North |366811.0    |326818  |9.12         |\n",
      "|1017     |Nagpur  |East  |658552.0    |280743  |96.78        |\n",
      "|1018     |Nagpur  |East  |938738.0    |218097  |40.11        |\n",
      "|1019     |Nashik  |West  |112412.0    |219553  |-47.5        |\n",
      "+---------+--------+------+------------+--------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_bank.show() # Muestra las primeras 20 filas\n",
    "df_bank.show(5) # Muestra las primeras 5 filas\n",
    "df_bank.show(truncate=False) # Evita truncar el contenido de las columnas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d187c45d",
   "metadata": {},
   "source": [
    "count(): Obtiene el número total de filas en el DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a32bb840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de filas en el DataFrame: 1000\n"
     ]
    }
   ],
   "source": [
    "num_rows = df_bank.count()\n",
    "print(f\"Número de filas en el DataFrame: {num_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97c2125",
   "metadata": {},
   "source": [
    "columns: Obtiene una lista de los nombres de las columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac131997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombres de las columnas: ['Branch_ID', 'City', 'Region', 'Firm_Revenue', 'Expenses', 'Profit_Margin']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Nombres de las columnas: {df_bank.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aea397a",
   "metadata": {},
   "source": [
    "Ejemplos Prácticos en Clase:\n",
    "\n",
    "Montar el entorno:\n",
    "Instalar pyspark: !pip install pyspark.\n",
    "Subir bank_data.csv al entorno.\n",
    "Crear la SparkSession.\n",
    "Cargar bank_data.csv: Demostrar cómo se carga, y qué sucede si header=False o inferSchema=False.\n",
    "Analizar printSchema(): Explicar los tipos de datos inferidos por Spark y qué significa nullable=true.\n",
    "Visualizar con show(): Mostrar diferentes usos de show() para ver las primeras filas y los datos completos.\n",
    "Contar filas y listar columnas: Demostrar count() y columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12be0f18",
   "metadata": {},
   "source": [
    "10 Ejercicios para la Estudiante (Requerimientos Laborales Simulados):\n",
    "\n",
    "Contexto General: Eres un analista de datos en el Banco de Inversiones y necesitas preparar y entender nuestros datos fundamentales.\n",
    "\n",
    "Instrucciones: Utiliza el archivo bank_data.csv para estos ejercicios. Si necesitas más datos, puedes generar versiones más grandes del CSV para simular Big Data.\n",
    "\n",
    "Inicialización y Naming:\n",
    "Requerimiento: Nuestro equipo de DevOps necesita que cada script de Spark tenga un nombre de aplicación claro para monitoreo.\n",
    "Ejercicio: Crea una SparkSession y asígnale el nombre \"Reporte_Rentabilidad_Sucursales\".\n",
    "Carga de Datos de Sucursales:\n",
    "Requerimiento: Queremos cargar el registro de nuestras sucursales para un nuevo análisis. Asegúrate de que las cabeceras se reconozcan y que Spark infiera los tipos de datos automáticamente.\n",
    "Ejercicio: Carga el archivo bank_data.csv en un DataFrame llamado df_sucursales.\n",
    "Validación de Esquema Inicial:\n",
    "Requerimiento: El equipo de calidad de datos necesita una validación rápida de los tipos de datos de las columnas para asegurar que la información se está cargando correctamente.\n",
    "Ejercicio: Muestra el esquema (printSchema()) del df_sucursales para verificar los tipos de datos inferidos para Branch_ID, Firm_Revenue y Profit_Margin. ¿Son los esperados (ej. numéricos para ingresos/márgenes)?\n",
    "Revisión de Datos Críticos:\n",
    "Requerimiento: El gerente de operaciones bancarias quiere ver los primeros registros para asegurarse de que los datos de ingresos y gastos de las sucursales se vean coherentes.\n",
    "Ejercicio: Muestra las primeras 5 filas del df_sucursales para una inspección visual.\n",
    "Verificación de Datos Completos:\n",
    "Requerimiento: Antes de iniciar cualquier análisis, necesitamos saber el volumen exacto de registros de sucursales que tenemos.\n",
    "Ejercicio: Obtén y muestra el número total de filas (registros de sucursales) en el df_sucursales.\n",
    "Inspección de Encabezados:\n",
    "Requerimiento: Para fines de documentación interna, necesitamos una lista de los nombres exactos de todas las columnas en nuestro DataFrame de sucursales.\n",
    "Ejercicio: Imprime la lista de todos los nombres de las columnas del df_sucursales.\n",
    "Carga con Especificación Manual (Simulando un caso de error):\n",
    "Requerimiento (hipotético): Imagina que el archivo bank_data.csv no tiene encabezado. ¿Cómo lo cargarías?\n",
    "Ejercicio: Carga bank_data.csv en un nuevo DataFrame (df_sin_header) asumiendo que no tiene encabezado (header=False). ¿Cómo se ven los nombres de las columnas ahora? (No es necesario corregirlos, solo observarlos).\n",
    "Comparación de Esquemas:\n",
    "Requerimiento: Compara el esquema del df_sucursales (con header=True, inferSchema=True) con el df_sin_header.\n",
    "Ejercicio: Muestra los esquemas de ambos DataFrames y explica brevemente la diferencia clave en los nombres de las columnas.\n",
    "Preparación para Siguiente Clase:\n",
    "Requerimiento: Para el próximo análisis, necesitamos el DataFrame principal de sucursales cargado y listo.\n",
    "Ejercicio: Asegúrate de que tu SparkSession está activa y que df_sucursales (el que tiene header=True y inferSchema=True) está disponible para su uso. (No hay código nuevo, es una verificación).\n",
    "Liberar Recursos:\n",
    "Requerimiento: Para mantener limpios nuestros entornos de desarrollo, es una buena práctica detener la SparkSession cuando ya no se necesita.\n",
    "Ejercicio: Detén tu SparkSession de Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e7143a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ffe1a01d",
   "metadata": {},
   "source": [
    "Recursos Adicionales para la Alumna:\n",
    "\n",
    "Documentación oficial de PySpark: https://spark.apache.org/docs/latest/api/python/index.html (¡muy útil para consultar funciones!)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
